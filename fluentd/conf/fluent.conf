# # # # Fluentd main configuration file
# # # # Reference: https://docs.fluentd.org/v1.0/articles/config-file

# # # # Set Fluentd to listen via http on port 8080, listening on all hosts
# # # <source>
# # #   @type http
# # #   port 8080
# # #   bind 0.0.0.0
# # # </source>

# # # # Events having prefix 'myapp.' will be stored both on Elasticsearch and files.
# # # <match myapp**.**>
# # #   @type copy
# # #   <store>
# # #     @type elasticsearch
# # #     host elasticsearch
# # #     port 9200
# # #     index_name fluentd
# # #     type_name fluentd
# # #     logstash_format true
# # #     logstash_prefix fluentd
# # #     logstash_dateformat %Y%m%d
# # #     include_tag_key true
# # #     tag_key @log_name
# # #     flush_interval 1s
# # #   </store>
# # #   <store>
# # #     @type file
# # #     path /logs/myapp
# # #     flush_interval 30s
# # #   </store>
# # # </match>

# # # # All other events will be printed to stdout
# # # <match **>
# # #   @type stdout
# # # </match>


# # # system.conf: |-
# # <system>
# #   root_dir /tmp/fluentd-buffers/
# # </system>

# # # system.input.conf: |-

# # # SOURCE: https://github.com/adriansr/beats/blob/938b0322a993021d79ed2a8be24055a7f3e69300/x-pack/filebeat/module/iptables/log/test/ubiquiti.log
# # # EXAMPLE: Jan  5 20:17:05 MainFirewall kernel: [LAN_LOCAL-default-A]IN=eth0.90 OUT= MAC=90:10:92:6e:ea:a7:90:10:73:ba:d6:77:08:00:45:fc:02:1c SRC=192.168.48.137 DST=255.55.174.225 LEN=540 TOS=0x1C PREC=0xE0 TTL=64 ID=27223 PROTO=UDP SPT=48689 DPT=48689 LEN=520
# # # EXAMPLE: Jan  5 20:17:01 MainFirewall kernel: [WAN_OUT-2000-A]IN=eth0 OUT=eth2 MAC=90:10:20:76:8d:20:90:10:24:67:f4:89:08:00 SRC=192.168.134.158 DST=192.0.2.25 LEN=265 TOS=0x00 PREC=0x00 TTL=63 ID=51768 DF PROTO=TCP SPT=43189 DPT=443 WINDOW=159 RES=0x00 ACK PSH URGP=0
# # # EXAMPLE: Jan  5 20:17:01 MainFirewall kernel: [source-dest-default-D]IN=eth0 OUT=eth2 MAC=90:10:20:76:8d:20:90:10:65:29:b6:2a:08:00 SRC=192.168.110.116 DST=192.0.2.25 LEN=52 TOS=0x00 PREC=0x00 TTL=63 ID=0 DF PROTO=TCP SPT=50093 DPT=1443 WINDOW=2857 RES=0x00 ACK URGP=0
# # # EXAMPLE: Jan  5 20:17:01 MainFirewall kernel: [WAN_OUT-2000-A]IN=eth0 OUT=eth2 MAC=90:10:20:76:8d:20:90:10:65:29:b6:2a:08:00 SRC=192.168.110.116 DST=192.0.2.25 LEN=52 TOS=0x00 PREC=0x00 TTL=63 ID=0 DF PROTO=TCP SPT=50093 DPT=1443 WINDOW=2853 RES=0x00 ACK URGP=0
# # # EXAMPLE: Jan  5 20:17:01 MainFirewall kernel: [WAN_OUT-2000-A]IN=eth0 OUT=eth2 MAC=90:10:20:76:8d:20:90:10:65:29:b6:2a:08:00 SRC=192.168.110.116 DST=192.0.2.25 LEN=52 TOS=0x00 PREC=0x00 TTL=63 ID=0 DF PROTO=TCP SPT=50093 DPT=1443 WINDOW=2850 RES=0x00 ACK URGP=0
# # # <source>
# # #   @type tail
# # #   @id ingest_from_unifi
# # #   @label @ingest_from_unifi
# # #   read_from_head true
# # #   tag ingest_from_unifi
# # #   pos_file /log/fluentd.pos
# # #   # format none
# # #   path /log/client_logs/**/*.log
# # #   tag unifi_syslog
# # #   # <parse>
# # #   #   @type grok
# # #   #   <grok>
# # #   #     custom_pattern_path /grok.d
# # #   #     pattern %{SYSLOG5424PRI}%{SYSLOGTIMESTAMP:syslog_timestamp}%{UNIFI_DEV_TYPE} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}\n?
# # #   #     time_format %m%d %H:%M:%S.%N
# # #   #   </grok>
# # #   # </parse>
# # # </source>


# # # <source>
# # #   @type tail
# # #   @id ingest_from_unifiserver
# # #   @label @ingest_from_unifiserver
# # #   read_from_head true
# # #   tag ingest_from_unifiserver
# # #   pos_file /log/fluentd.pos
# # #   # format none
# # #   path /log/client_logs/**/*.log
# # #   tag unifiserver_syslog
# # #   <parse>
# # #     @type grok
# # #     <grok>
# # #       pattern %{TIME:time}\\] <%{WORD}-%{WORD}> %{WORD:LEVEL} %{SPACE} %{DATA} - from \\[%{DATA:CLIENTMAC}\\]\\(%{GREEDYDATA:MSG}\\): state=%{WORD:state}
# # #       time_format %m%d %H:%M:%S.%N
# # #       custom_pattern_path /grok.d
# # #     </grok>
# # #   </parse>
# # # </source>

# # # <source>
# # #   @type tail
# # #   @id ingest_from_unifi_iptables
# # #   @label @ingest_from_unifi_iptables
# # #   read_from_head true
# # #   tag ingest_from_unifi_iptables
# # #   pos_file /log/fluentd.pos
# # #   # format none
# # #   path /log/client_logs/**/*.log
# # #   tag unifi_iptables_syslog
# # #   custom_pattern_path /grok.d
# # #   <parse>
# # #     @type grok
# # #     <grok>
# # #       pattern %{SYSLOGTIMESTAMP:iptables.raw_date}%{GREEDYDATA}\\[%{UBIQUITI_LABEL}\\]%{IPTABLES}%{SPACE}
# # #       time_format %m%d %H:%M:%S.%N
# # #       custom_pattern_path /grok.d
# # #     </grok>
# # #     <grok>
# # #       pattern %{SYSLOGTIMESTAMP:iptables.raw_date}%{GREEDYDATA}%{IPTABLES}%{SPACE}
# # #       time_format %m%d %H:%M:%S.%N
# # #       custom_pattern_path /grok.d
# # #     </grok>
# # #     <grok>
# # #       pattern %{GREEDYDATA}\\[%{UBIQUITI_LABEL}\\]%{IPTABLES}%{SPACE}
# # #       time_format %m%d %H:%M:%S.%N
# # #       custom_pattern_path /grok.d
# # #     </grok>
# # #     <grok>
# # #       pattern %{GREEDYDATA}%{IPTABLES}%{SPACE}
# # #       time_format %m%d %H:%M:%S.%N
# # #       custom_pattern_path /grok.d
# # #     </grok>
# # #   </parse>
# # # </source>

# # # NOTE: This was the only one that was working!!!!! 5/1/2019
# # # NOTE: This was the only one that was working!!!!! 5/1/2019
# # # NOTE: This was the only one that was working!!!!! 5/1/2019
# # # NOTE: This was the only one that was working!!!!! 5/1/2019
# # # NOTE: This was the only one that was working!!!!! 5/1/2019
# # # NOTE: This was the only one that was working!!!!! 5/1/2019
# # # DISABLED: <source>
# # # DISABLED:   @type tail
# # # DISABLED:   @id ingest_from_unifi_iptables_ubiquiti_rule_set
# # # DISABLED:   @label @ingest_from_unifi_iptables_ubiquiti_rule_set
# # # DISABLED:   read_from_head true
# # # DISABLED:   tag ingest_from_unifi_iptables_ubiquiti_rule_set
# # # DISABLED:   pos_file /log/fluentd.pos
# # # DISABLED:   # format none
# # # DISABLED:   path /log/client_logs/**/*.log
# # # DISABLED:   tag unifi
# # # DISABLED:   <parse>
# # # DISABLED:     @type grok
# # # DISABLED:     custom_pattern_path /grok.d
# # # DISABLED:     # pattern %{UBIQUITI_FIELD_IPTABLES_RULE_SET:iptables.ubiquiti.input_zone}-%{UBIQUITI_FIELD_IPTABLES_RULE_SET:iptables.ubiquiti.output_zone}
# # # DISABLED:     pattern %{UBIQUITI_FIELD_IPTABLES_RULE_SET:iptables.ubiquiti.input_zone}
# # # DISABLED:     grok_pattern %{UBIQUITI_FIELD_IPTABLES_RULE_SET:iptables.ubiquiti.input_zone}
# # # DISABLED:     time_format %m%d %H:%M:%S.%N
# # # DISABLED:     time_key timestamp
# # # DISABLED:   </parse>
# # # DISABLED: </source>

# # # EXAMPLE: log file produced
# # # 2019-04-16 04:18:57.000000000 +0000 unifi.syslog.client_logs.unifisecuritygateway3p.messages.log:
# # # {
# # #     "hostname": "UniFiSecurityGateway3P",
# # #     "process_name": "kernel",
# # #     "firewall.interface": "LAN_LOCAL",
# # #     "firewall.rule_index": "default",
# # #     "firewall.rule_action": "A",
# # #     "iptables.input_device": "eth1",
# # #     "iptables.output_device": "",
# # #     "mac": "80:2a:a8:00:11:33:00:11:22",
# # #     "destination.mac": "33:44:66:08:00",
# # #     "source.mac": "80:2a:a8:00:00",
# # #     "iptables.ether_type": "08:00",
# # #     "firewall.source.ip": "192.168.1.172",
# # #     "firewall.destination.ip": "192.168.1.1",
# # #     "firewall.packet_length": "72",
# # #     "firewall.tos": "00",
# # #     "firewall.precidence_field": "00",
# # #     "firewall.ttl": "63",
# # #     "firewall.id": "22949",
# # #     "firewall.tcp_opts": "LEN=52 ",
# # #     "firewall.nf_protocol": "UDP",
# # #     "firewall.spt": "25761",
# # #     "firewall.dtp": "53"
# # # }

# # <source>
# #   @type tail
# #   @id ingest_dummy_logs
# #   read_from_head true
# #   path /log/client_logs/dummy.log
# #   tag unifi_dummy_logs
# #   # dummy May 18 20:54:31 UniFiSecurityGateway3P kernel: [WAN_LOCAL-4000-D]IN=eth0 OUT= MAC=80:2a:a8:00:11:33:00:11:22:33:44:66:08:00 SRC=172.217.10.138 DST=192.168.0.3 LEN=115 TOS=0x00 PREC=0x00 TTL=122 ID=53703 PROTO=TCP SPT=443 DPT=52170 WINDOW=294 RES=0x00 ACK PSH URGP=0
# #   <parse>
# #     @type grok
# #     custom_pattern_path /grok.d
# #     grok_pattern %{BOSSJONES_UNIFI_SECURITYGATEWAY_KERNEL_START} (?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_LOGS})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_IPTABLES_ETHERNET}|%{BOSSJONES_IPTABLES_IP_START})? (?:%{BOSSJONES_IPTABLES_IP_START})?
# #     #time_type string
# #     # NOTE: THIS WORKED BEFORE #time_key timestamp
# #     time_key time
# #     time_format %b %d %H:%M:%S
# #     keep_time_key true
# #   </parse>
# # </source>

# # <source>
# #   @type tail
# #   @id ingest_from_unifi_iptables_ubiquiti_rule_set
# #   # @label @ingest_from_unifi_iptables_ubiquiti_rule_set
# #   read_from_head true
# #   # tag ingest_from_unifi_iptables_ubiquiti_rule_set
# #   pos_file /log/fluentd.pos
# #   # format none
# #   # time_type string
# #   # time_format +%b %d %H:%M:%S
# #   path /log/client_logs/**/*.log
# #   tag unifi
# #   <parse>
# #     @type grok
# #     # grok_name_key grok_name
# #     # grok_failure_key grokfailure
# #     custom_pattern_path /grok.d

# #     # pattern %{BOSSJONES_UNIFI_SECURITYGATEWAY_KERNEL_START} %{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_LOGS}%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_IPTABLES_ETHERNET} %{BOSSJONES_IPTABLES_IP_START}
# #     grok_pattern %{BOSSJONES_UNIFI_SECURITYGATEWAY_KERNEL_START} (?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_LOGS})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_IPTABLES_ETHERNET}|%{BOSSJONES_IPTABLES_IP_START})? (?:%{BOSSJONES_IPTABLES_IP_START})?
# #     #time_type string
# #     # NOTE: THIS WORKED BEFORE #time_key timestamp
# #     time_key time
# #     time_format %b %d %H:%M:%S
# #     keep_time_key true
# #     # EXAMPLE: Apr 20 05:51:40 UniFiSecurityGateway3P sudo: pam_unix(sudo:session): session closed for user root
# #     # 2019-05-02 22:28:09.449511513 +0000
# #     # format json
# #     # time_key timestamp_logged
# #   </parse>
# # </source>

# # # filter.conf: |-
# # # exclude all commands that have pam_unix in them

# # # <filter **>
# # #   @type parser
# # #   @id exclude_pam_unix
# # #   <parse>
# # #     @type grok
# # #     grok_failure_key grokfailure
# # #     <grok>
# # #       pattern %{PATH:path}
# # #     </grok>
# # #   </parse>
# # #   <exclude>
# # #     key message
# # #     pattern pam_unix
# # #   </exclude>
# # # </filter>

# # <filter **>
# #   @type grep
# #   @id exclude_pam_unix
# #   <exclude>
# #     key message
# #     pattern pam_unix
# #   </exclude>
# # </filter>

# # <filter **>
# #   @type grep
# #   @id exclude_mcad
# #   <exclude>
# #     key message
# #     pattern mcad
# #   </exclude>
# # </filter>

# # <filter **>
# #   @type grep
# #   @id exclude_ipsec_statusall
# #   <exclude>
# #     key message
# #     pattern sudo:     root : TTY=unknown ; PWD=/ ; USER=root ; COMMAND=/usr/sbin/ipsec statusall
# #   </exclude>
# # </filter>

# # # <filter unifi>
# # #   @type grep
# # #   @id filter_iptables_deny_on_wan_local
# # #   <regexp>
# # #     key firewall.interface
# # #     pattern WAN_LOCAL
# # #   </regexp>
# # #   <regexp>
# # #     key firewall.rule_action
# # #     pattern D
# # #   </regexp>
# # #   tag remote_unifi
# # # </filter>

# # # <label @unifi>
# # # <filter ingest_from_unifi_iptables_ubiquiti_rule_set**.**>
# # <filter **>
# #   @type    geoip
# #   # Specify optional geoip2 database
# #   geoip2_database   "/geoip/GeoLite2-City.mmdb"
# #   # Specify backend library (geoip2_c, geoip, geoip2_compat)
# #   backend_library geoip2_c


# #   geoip_lookup_keys  firewall.source.ip

# #   <record>
# #     firewall.city            ${city.names.en["firewall.source.ip"]}
# #     firewall.latitude        ${location.latitude["firewall.source.ip"]}
# #     firewall.longitude       ${location.longitude["firewall.source.ip"]}
# #     firewall.longitude       ${location.longitude["firewall.source.ip"]}
# #     firewall.src_geo_location    '{ "lat" : ${location.latitude["firewall.source.ip"]}, "lon" : ${location.longitude["firewall.source.ip"]} }'
# #     firewall.country         ${country.iso_code["firewall.source.ip"]}
# #     firewall.country_name    ${country.names.en["firewall.source.ip"]}
# #     firewall.postal_code     ${postal.code["firewall.source.ip"]}
# #   </record>

# #   # To avoid get stacktrace error with `[null, null]` array for elasticsearch.
# #   skip_adding_null_record  true

# #   # Set @log_level (default: warn)
# #   @log_level         debug
# # </filter>
# # # </label>


# # # <filter access.apache>
# # #   @type                  geoip
# # #   geoip_lookup_keys      host
# # #   <record>
# # #     # lat lon as properties
# # #     # ex. {"lat" => 37.4192008972168, "lon" => -122.05740356445312 }
# # #     location_properties  '{ "lat" : ${location.latitude["host"]}, "lon" : ${location.longitude["host"]} }'

# # #     # lat lon as string
# # #     # ex. "37.4192008972168,-122.05740356445312"
# # #     location_string      ${location.latitude["host"]},${location.longitude["host"]}

# # #     # GeoJSON (lat lon as array) is useful for Kibana's bettermap.
# # #     # ex. [-122.05740356445312, 37.4192008972168]
# # #     location_array       '[${location.longitude["host"]},${location.latitude["host"]}]'
# # #   </record>

# # #   # To avoid get stacktrace error with `[null, null]` array for elasticsearch.
# # #   skip_adding_null_record  true
# # # </filter>

# # <filter access>
# #   @type record_transformer
# #   <record>
# #     hostname ${hostname}
# #   </record>
# # </filter>


# # #  monitoring.conf: |-
# # # Prometheus Exporter Plugin
# # # input plugin that exports metrics
# # <source>
# #   @type prometheus
# # </source>
# # <source>
# #   @type monitor_agent
# # </source>
# # # input plugin that collects metrics from MonitorAgent
# # <source>
# #   @type prometheus_monitor
# #   <labels>
# #     host ${hostname}
# #   </labels>
# # </source>
# # # input plugin that collects metrics for output plugin
# # <source>
# #   @type prometheus_output_monitor
# #   <labels>
# #     host ${hostname}
# #   </labels>
# # </source>
# # # input plugin that collects metrics for in_tail plugin
# # <source>
# #   @type prometheus_tail_monitor
# #   <labels>
# #     host ${hostname}
# #   </labels>
# # </source>


# # #  output.conf: |-
# # # <label @ingest_from_unifi>
# # #   <match **>
# # #     @type stdout
# # #     output_type json
# # #   </match>
# # # </label>

# # # <label @ingest_from_unifiserver>
# # #   <match **>
# # #     @type stdout
# # #     output_type json
# # #   </match>
# # # </label>

# # # <label @ingest_from_unifi_iptables>
# # #   <match **>
# # #     @type stdout
# # #     output_type json
# # #   </match>
# # # </label>

# # # <label @ingest_from_unifi_iptables_ubiquiti_rule_set>
# # #   <match **>
# # #     @type stdout
# # #     output_type json
# # #   </match>
# # # </label>

# # # <label @unifi>

# # #   <match **>
# # #     @type copy
# # #     <store>
# # #       @id elasticsearch
# # #       @type elasticsearch
# # #       @log_level info
# # #       include_tag_key true
# # #       host elasticsearch-logging
# # #       port 9200
# # #       logstash_format true
# # #       <buffer>
# # #         @type file
# # #         path /var/log/fluentd-buffers/kubernetes.system.buffer
# # #         flush_mode interval
# # #         retry_type exponential_backoff
# # #         flush_thread_count 2
# # #         flush_interval 5s
# # #         retry_forever
# # #         retry_max_interval 30
# # #         chunk_limit_size 2M
# # #         queue_limit_length 8
# # #         overflow_action block
# # #       </buffer>
# # #     </store>
# # #     <store>
# # #       @type stdout
# # #     </store>
# # #   </match>
# # # </label>

# # # <label @ingest_from_unifi_iptables_ubiquiti_rule_set>
# # <match **>
# #   @type copy
# #   # <store>
# #   #   @id elasticsearch
# #   #   @type elasticsearch
# #   #   @log_level debug
# #   #   with_transporter_log true
# #   #   type_name fluentd
# #   #   include_tag_key true
# #   #   # host elasticsearch.borglab.scarlettlab.home
# #   #   host elasticsearch
# #   #   port 9200
# #   #   logstash_format true
# #   #   # emit_error_for_missing_id true
# #   #   # scheme http
# #   #   # time_key @timestamp
# #   #   # index_name unifi
# #   #   # logstash_prefix unifi
# #   #   # include_timestamp true
# #   #   reload_on_failure true
# #   #   reconnect_on_error true
# #   #   # template_file /etc/fluent/config.d/elasticsearch-template-es5x.json
# #   #   # template_name elasticsearch-template-es5x.json
# #   #   <buffer>
# #   #     @type file
# #   #     path /var/log/fluentd.centralized.system.buffer
# #   #     flush_mode interval
# #   #     retry_type exponential_backoff
# #   #     flush_thread_count 2
# #   #     flush_interval 5s
# #   #     retry_forever
# #   #     retry_max_interval 30
# #   #     chunk_limit_size 4M
# #   #     queue_limit_length 256
# #   #     overflow_action block
# #   #   </buffer>
# #   # </store>
# #   <store>
# #     @type stdout
# #     output_type json
# #   </store>
# #   <store>
# #     @type flowcounter
# #     @id applog_high_metrics
# #     tag fluentd.apploghigh.metrics
# #     aggregate all
# #     count_keys *
# #     unit minute
# #   </store>
# # </match>
# # # </label>





# <match fluent.**>
#   # this tells fluentd to not output its log on stdout
#   @type null
# </match>

# # system.conf: |-
# <system>
#   root_dir /tmp/fluentd-buffers/
# </system>
# # system.input.conf: |-
# # <source>
# #   @type tail
# #   @id ingest_dummy_logs
# #   read_from_head true
# #   path /log/client_logs/dummy.log
# #   tag unifi_dummy_logs**.**
# #   # dummy May 18 20:54:31 UniFiSecurityGateway3P kernel: [WAN_LOCAL-4000-D]IN=eth0 OUT= MAC=80:2a:a8:00:11:33:00:11:22:33:44:66:08:00 SRC=172.217.10.138 DST=192.168.0.3 LEN=115 TOS=0x00 PREC=0x00 TTL=122 ID=53703 PROTO=TCP SPT=443 DPT=52170 WINDOW=294 RES=0x00 ACK PSH URGP=0
# #   <parse>
# #     @type grok
# #     custom_pattern_path /grok.d
# #     grok_pattern %{BOSSJONES_UNIFI_SECURITYGATEWAY_KERNEL_START} (?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_LOGS})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_IPTABLES_ETHERNET}|%{BOSSJONES_IPTABLES_IP_START})? (?:%{BOSSJONES_IPTABLES_IP_START})?
# #     #time_type string
# #     # NOTE: THIS WORKED BEFORE #time_key timestamp
# #     time_key time
# #     time_format %b %d %H:%M:%S
# #     keep_time_key true
# #   </parse>
# # </source>

# <source>
#   @type tail
#   @id ingest_from_unifi_iptables_ubiquiti_rule_set
#   read_from_head true
#   # tag ingest_from_unifi_iptables_ubiquiti_rule_set
#   # pos_file /log/fluentd.pos
#   path /log/client_logs/**/*.log
#   tag unifi.syslog.**
#   # tag unifi.syslog
#   <parse>
#     @type grok
#     custom_pattern_path /grok.d
#     grok_pattern %{BOSSJONES_UNIFI_RAW}
#     #  grok_pattern %{BOSSJONES_UNIFI_SECURITYGATEWAY_KERNEL_START} (?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_IDS_LOGS_START})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_LOGS})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_IDS_LOGS_START})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_LOGS})?(?:%{SPACE})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_IPTABLES_ETHERNET}|%{BOSSJONES_IPTABLES_IP_START})? (?:%{BOSSJONES_IPTABLES_IP_START})?
#     time_key time
#     time_format %b %d %H:%M:%S
#     keep_time_key true
#   </parse>
# </source>



# # # Detect exceptions in the log output and forward them as one log entry.
# # <match unifi.syslog.**>
# #   @id unifi
# #   @type detect_exceptions
# #   remove_tag_prefix raw
# #   message log
# #   stream stream
# #   multiline_flush_interval 5
# #   max_bytes 500000
# #   max_lines 1000
# # </match>

# <filter unifi.syslog.**>
#   @type parser
#   @id iptables_security_gateway_logs
#   key_name message
#   reserve_data true
#   reserve_time true
#   # remove_key_name_field true
#   <parse>
#     @type grok
#     custom_pattern_path /grok.d
#     grok_failure_key grokfailure
#     # WITHOUT IDS BLOCK
#     <grok>
#       pattern (?:%{SPACE})?%{BOSSJONES_UNIFI_PROCESS_NAME}(?:%{SPACE})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_LOGS})(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_LOGS})?(?:%{SPACE})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_IPTABLES_ETHERNET}|%{BOSSJONES_IPTABLES_IP_START})? (?:%{BOSSJONES_IPTABLES_IP_START})?
#     </grok>
#     # IDS BLOCK
#     # EG. Apr 17 21:10:16 UniFiSecurityGateway3P kernel: ALIEN BLOCK: IN=eth0 OUT=eth1 MAC=ff:ff:f2:fe:f0:fc:00:0f:29:08:ff:f2:08:00 SRC=46.246.123.145 DST=192.168.1.24 LEN=29 TOS=0x00 PREC=0x00 TTL=49 ID=4452 DF PROTO=UDP SPT=8888 DPT=60156 LEN=9
#     <grok>
#       pattern (?:%{SPACE})?%{BOSSJONES_UNIFI_PROCESS_NAME}(?:%{SPACE})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_LOGS})?%{BOSSJONES_UNIFI_SECURITYGATEWAY_IDS_LOGS_START}(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_LOGS})?(?:%{SPACE})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_IPTABLES_ETHERNET}|%{BOSSJONES_IPTABLES_IP_START})? (?:%{BOSSJONES_IPTABLES_IP_START})?
#     </grok>
#     # IN= then SRC=
#     # EG. kernel: [LAN_LOCAL-default-A]IN=eth1 OUT= MAC=ff:ff:f2:fe:f0:fc:00:0f:29:08:ff:f2:08:00 SRC=192.168.1.172 DST=192.168.1.1 LEN=119 TOS=0x00 PREC=0x00 TTL=63 ID=21542 DF PROTO=UDP SPT=5432 DPT=53 LEN=99
#     <grok>
#       pattern (?:%{SPACE})?%{BOSSJONES_UNIFI_PROCESS_NAME}(?:%{SPACE})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_LOGS})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_LOGS})?(?:%{SPACE})?%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_IPTABLES_ETHERNET} (?:%{BOSSJONES_IPTABLES_IP_START})?
#     </grok>
#     <grok>
#       pattern %{GREEDYDATA:raw_log}
#     </grok>
#   </parse>
# </filter>

# # <match unifi.syslog.**>
# #   @type file
# #   path /log/dummy.log
# # </match>

# # # DEBUG
# # <match unifi.syslog.**>
# #   @type stdout
# # </match>

# # <filter unifi.syslog.**>
# #   @type parser
# #   @id iptables_security_gateway_logs
# #   key_name message
# #   reserve_data true
# #   reserve_time true
# #   remove_key_name_field true
# #   # hash_value_field ids
# #   # @label @ids
# #   <parse>
# #     @type grok
# #     custom_pattern_path /grok.d
# #     grok_failure_key grokfailure
# #     # <grok>
# #     #   pattern (?:%{SPACE})?%{BOSSJONES_UNIFI_PROCESS_NAME}(?:%{SPACE})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_LOGS})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_IDS_LOGS_START})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_LOGS})?(?:%{SPACE})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_IPTABLES_ETHERNET}|%{BOSSJONES_IPTABLES_IP_START})? (?:%{BOSSJONES_IPTABLES_IP_START})?
# #     # </grok>
# #     <grok>
# #       pattern (?:%{BOSSJONES_UNIFI_PROCESS_NAME})?(?:%{SPACE})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_LOGS})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_IDS_LOGS_START})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_LOGS})?(?:%{SPACE})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_IPTABLES_ETHERNET}|%{BOSSJONES_IPTABLES_IP_START})? (?:%{BOSSJONES_IPTABLES_IP_START})?
# #     </grok>
# #     # <grok>
# #     #   name log_message
# #     #   pattern %{GREEDYDATA:log_message}
# #     # </grok>
# #   </parse>
# # </filter>


# # <match unifi.syslog.**>
# #   @type stdout
# # </match>


# # filter.conf: |-
# # exclude all commands that have pam_unix in them
# <filter unifi.syslog.**>
#   @type grep
#   @id exclude_pam_unix
#   <exclude>
#     key message
#     pattern pam_unix
#   </exclude>
# </filter>
# <filter unifi.syslog.**>
#   @type grep
#   @id exclude_mcad
#   <exclude>
#     key message
#     pattern mcad
#   </exclude>
# </filter>
# <filter unifi.syslog.**>
#   @type grep
#   @id exclude_ipsec_statusall
#   <exclude>
#     key message
#     pattern sudo:     root : TTY=unknown ; PWD=/ ; USER=root ; COMMAND=/usr/sbin/ipsec statusall
#   </exclude>
# </filter>


# # UNIFI SYSLOG ONLY

# # <filter unifi.syslog.**>
# #   @type grep
# #   @id only_wan_local
# #   <regexp>
# #     key firewall.interface
# #     pattern WAN_LOCAL
# #   </regexp>
# # </filter>

# <filter unifi.syslog.**>
#   @type record_transformer
#   enable_ruby true
#   <record>
#     # NOTE: If null, set default value to 127.0.0.1
#     firewall_source_ip ${record.dig('firewall.source.ip') ? record.dig('firewall.source.ip') : ''}
#   </record>
# </filter>

# # {
# #   "found": true,
# #   "_index": "my_index",
# #   "_type": "my_type",
# #   "_id": "my_id",
# #   "_version": 1,
# #   "_source": {
# #     "ip": "8.8.8.8",
# #     "geoip": {
# #       "continent_name": "North America",
# #       "country_iso_code": "US",
# #       "region_name": "California",
# #       "city_name": "Mountain View",
# #       "location": { "lat": 37.386, "lon": -122.0838 }
# #     }
# #   }
# # }

# # unifi.syslog.log.client_logs.unifisecuritygateway3p.messages.log
# <filter unifi.syslog.**>
#   @type    geoip
#   # Specify optional geoip2 database
#   geoip2_database   "/geoip/GeoLite2-City.mmdb"
#   # Specify backend library (geoip2_c, geoip, geoip2_compat)
#   backend_library geoip2_c
#   geoip_lookup_keys  ["firewall.source.ip"]
#   <record>
#     geoip.latitude         ${location.latitude["firewall.source.ip"]}
#     geoip.longitude        ${location.longitude["firewall.source.ip"]}

#     # lat lon as properties
#     # ex. {"lat" => 37.4192008972168, "lon" => -122.05740356445312 }
#     geoip.location    '{ "lat" : ${location.latitude["firewall.source.ip"]}, "lon" : ${location.longitude["firewall.source.ip"]} }'

#     # lat lon as string
#     # ex. "37.4192008972168,-122.05740356445312"
#     # geoip.location_string      ${location.latitude["firewall.source.ip"]},${location.longitude["firewall.source.ip"]}

#     # GeoJSON (lat lon as array) is useful for Kibana's bettermap.
#     # ex. [-122.05740356445312, 37.4192008972168]
#     # geoip.location_array  '[${location.longitude["firewall.source.ip"]},${location.latitude["firewall.source.ip"]}]'
#     # geoip.postal_code     ${postal.code["firewall.source.ip"]}
#     geoip.region_name     ${subdivisions.0.names.en["firewall.source.ip"]}
#     geoip.country_name    ${country.names.en["firewall.source.ip"]}
#     geoip.country_iso_code ${country.iso_code["firewall.source.ip"]}
#     geoip.city_name       ${city.names.en["firewall.source.ip"]}
#   </record>
#   # To avoid get stacktrace error with `[null, null]` array for elasticsearch.
#   skip_adding_null_record  true
#   # Set @log_level (default: warn)
#   @log_level         debug
# </filter>


# <filter unifi.syslog.**>
#   @type parser
#   @id exclude_nil_firewall_source_ip
#   key_name firewall_source_ip
#   reserve_data true
#   reserve_time true
#   remove_key_name_field true
#   emit_invalid_record_to_error false
#   <parse>
#     @type grok
#     custom_pattern_path /grok.d
#     <grok>
#       pattern %{IP:remote_ip}
#     </grok>
#   </parse>
# </filter>

# # for debug
# # <match kubernetes.var.log.containers.**fluentd**.log>
# #   @type null
# # </match>
# # <match unifi.syslog.**>
# #   @type stdout
# # </match>

# <filter access>
#   @type record_transformer
#   <record>
#     hostname ${hostname}
#   </record>
# </filter>
# # monitoring.conf: |-
# # Prometheus Exporter Plugin
# # input plugin that exports metrics
# <source>
#   @type prometheus
#   bind 0.0.0.0
#   port 24231
#   metrics_path /metrics
# </source>
# <source>
#   @type monitor_agent
# </source>
# # input plugin that collects metrics from MonitorAgent
# <source>
#   @type prometheus_monitor
#   <labels>
#     host ${hostname}
#   </labels>
# </source>
# # input plugin that collects metrics for output plugin
# <source>
#   @type prometheus_output_monitor
#   <labels>
#     host ${hostname}
#   </labels>
# </source>
# # input plugin that collects metrics for in_tail plugin
# <source>
#   @type prometheus_tail_monitor
#   <labels>
#     host ${hostname}
#   </labels>
# </source>

# # # Discard all log records that have an empty or missing message field
# # <filter *.splunk.**>
# #   @type grep
# #   @id filter_blank_message_lines
# #   regexp1 message \S+
# # </filter>


# # <match fluent.**>
# #   # this tells fluentd to not output its log on stdout
# #   @type null
# # </match>

# # output.conf: |-
# <match **.**>
#   @type copy
#   <store ignore_error>
#     @id elasticsearch
#     @type elasticsearch
#     @log_level debug
#     # with_transporter_log true
#     type_name fluentd
#     include_tag_key true
#     host elasticsearch
#     port 9200
#     logstash_format true
#     # emit_error_for_missing_id true
#     logstash_prefix logstash-unifi
#     index_prefix logstash-unifi
#     # log_es_400_reason true
#     reload_on_failure true
#     reconnect_on_error true
#     # scheme http
#     # time_key @timestamp
#     # index_name unifi
#     # include_timestamp true
#     template_file /etc/fluent/conf.d/elasticsearch-template-es5x.json
#     template_name elasticsearch-template-es5x.json
#     <buffer>
#       @type file
#       path /var/log/fluentd.centralized.system.buffer
#       flush_mode interval
#       retry_type exponential_backoff
#       flush_thread_count 2
#       flush_interval 5s
#       retry_forever
#       retry_max_interval 30
#       chunk_limit_size 4M
#       queue_limit_length 256
#       overflow_action block
#     </buffer>
#   </store>
#   # <store>
#   #   @type stdout
#   #   output_type json
#   # </store>
#   <store>
#     @type flowcounter
#     @id applog_high_metrics
#     tag fluentd.apploghigh.metrics
#     aggregate all
#     count_keys *
#     unit minute
#   </store>
# </match>


# # <match **>
# #   @id elasticsearch
# #   @type elasticsearch
# #   @log_level debug
# #   # with_transporter_log true
# #   type_name fluentd
# #   include_tag_key true
# #   host elasticsearch
# #   port 9200
# #   logstash_format true
# #   # emit_error_for_missing_id true
# #   logstash_prefix logstash-unifi
# #   index_prefix logstash-unifi
# #   # log_es_400_reason true
# #   reload_on_failure true
# #   reconnect_on_error true
# #   # scheme http
# #   # time_key @timestamp
# #   # index_name unifi
# #   # include_timestamp true
# #   # template_file /etc/fluent/config.d/elasticsearch-template-es5x.json
# #   # template_name elasticsearch-template-es5x.json
# #   <buffer>
# #     @type file
# #     path /var/log/fluentd.centralized.system.buffer
# #     flush_mode interval
# #     retry_type exponential_backoff
# #     flush_thread_count 2
# #     flush_interval 5s
# #     retry_forever
# #     retry_max_interval 30
# #     chunk_limit_size 4M
# #     queue_limit_length 256
# #     overflow_action block
# #   </buffer>
# # </match>


<match fluent.**>
  # this tells fluentd to not output its log on stdout
  @type null
</match>
# system.conf: |-
<system>
  root_dir /tmp/fluentd-buffers/
  # workers 4
</system>
# <worker 0>
<source>
  @type tail
  @id ingest_from_unifi_iptables_ubiquiti_rule_set
  @log_level debug
  read_from_head true
  # tag ingest_from_unifi_iptables_ubiquiti_rule_set
  # pos_file /log/fluentd.unifi.pos
  pos_file /var/log/fluentd.unifi.pos
  path /log/client_logs/**/*.log
  tag unifi.syslog.**
  <parse>
    @type grok
    custom_pattern_path /grok.d
    grok_pattern %{BOSSJONES_UNIFI_RAW}
    time_key time
    time_format %b %d %H:%M:%S
    keep_time_key true
  </parse>
</source>
# </worker>
# # Detect exceptions in the log output and forward them as one log entry.
# <match unifi.syslog.**>
#   @id unifi
#   @type detect_exceptions
#   remove_tag_prefix raw
#   message log
#   stream stream
#   multiline_flush_interval 5
#   max_bytes 500000
#   max_lines 1000
# </match>
<filter unifi.syslog.**>
  @type parser
  @id iptables_security_gateway_logs
  @log_level         debug
  key_name message
  reserve_data true
  reserve_time true
  # remove_key_name_field true
  <parse>
    @type grok
    custom_pattern_path /grok.d
    grok_failure_key grokfailure
    # WITHOUT IDS BLOCK
    <grok>
      pattern (?:%{SPACE})?%{BOSSJONES_UNIFI_PROCESS_NAME}(?:%{SPACE})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_LOGS})(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_LOGS})?(?:%{SPACE})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_IPTABLES_ETHERNET}|%{BOSSJONES_IPTABLES_IP_START})? (?:%{BOSSJONES_IPTABLES_IP_START})?
    </grok>
    # IDS BLOCK
    # EG. Apr 17 21:10:16 UniFiSecurityGateway3P kernel: ALIEN BLOCK: IN=eth0 OUT=eth1 MAC=ff:ff:f2:fe:f0:fc:00:0f:29:08:ff:f2:08:00 SRC=46.246.123.145 DST=192.168.1.24 LEN=29 TOS=0x00 PREC=0x00 TTL=49 ID=4452 DF PROTO=UDP SPT=8888 DPT=60156 LEN=9
    <grok>
      pattern (?:%{SPACE})?%{BOSSJONES_UNIFI_PROCESS_NAME}(?:%{SPACE})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_LOGS})?%{BOSSJONES_UNIFI_SECURITYGATEWAY_IDS_LOGS_START}(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_LOGS})?(?:%{SPACE})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_IPTABLES_ETHERNET}|%{BOSSJONES_IPTABLES_IP_START})? (?:%{BOSSJONES_IPTABLES_IP_START})?
    </grok>
    # IN= then SRC=
    # EG. kernel: [LAN_LOCAL-default-A]IN=eth1 OUT= MAC=ff:ff:f2:fe:f0:fc:00:0f:29:08:ff:f2:08:00 SRC=192.168.1.172 DST=192.168.1.1 LEN=119 TOS=0x00 PREC=0x00 TTL=63 ID=21542 DF PROTO=UDP SPT=5432 DPT=53 LEN=99
    <grok>
      pattern (?:%{SPACE})?%{BOSSJONES_UNIFI_PROCESS_NAME}(?:%{SPACE})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_LOGS})?(?:%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_LOGS})?(?:%{SPACE})?%{BOSSJONES_UNIFI_SECURITYGATEWAY_FIREWALL_IPTABLES_ETHERNET} (?:%{BOSSJONES_IPTABLES_IP_START})?
    </grok>
    <grok>
      pattern %{GREEDYDATA:raw_log}
    </grok>
  </parse>
</filter>
# <match unifi.syslog.**>
#   @type stdout
# </match>
# filter.conf: |-
# exclude all commands that have pam_unix in them
<filter unifi.syslog.**>
  @type grep
  @id exclude_pam_unix
  <exclude>
    key message
    pattern pam_unix
  </exclude>
</filter>
<filter unifi.syslog.**>
  @type grep
  @id exclude_mcad
  <exclude>
    key message
    pattern mcad
  </exclude>
</filter>
<filter unifi.syslog.**>
  @type grep
  @id exclude_ipsec_statusall
  <exclude>
    key message
    pattern sudo:     root : TTY=unknown ; PWD=/ ; USER=root ; COMMAND=/usr/sbin/ipsec statusall
  </exclude>
</filter>
# UNIFI SYSLOG ONLY
# <filter unifi.syslog.**>
#   @type grep
#   @id only_wan_local
#   <regexp>
#     key firewall.interface
#     pattern WAN_LOCAL
#   </regexp>
# </filter>

# The CPU usage decreased from 70% to 30% of one CPU core after I added the wildcard to buffer_path.

# This means 1000 msg/sec by in_dummy and fake output combo consumes 70% CPU, right?

# I will test that a bit in our formal pipeline to see if other performance criteria is met.

# If your pipeline uses record_transformer with enable_ruby, replacing it with record_modifier also helps CPU usage.
# IMPORTANT: https://github.com/fluent/fluentd/issues/2236

<filter unifi.syslog.**>
  @type record_modifier
  @log_level         debug
  enable_ruby true
  <record>
    # NOTE: If null, set default value to 127.0.0.1
    firewall_source_ip ${record.dig('firewall.source.ip') ? record.dig('firewall.source.ip') : ''}
  </record>
</filter>
# {
#   "found": true,
#   "_index": "my_index",
#   "_type": "my_type",
#   "_id": "my_id",
#   "_version": 1,
#   "_source": {
#     "ip": "8.8.8.8",
#     "geoip": {
#       "continent_name": "North America",
#       "country_iso_code": "US",
#       "region_name": "California",
#       "city_name": "Mountain View",
#       "location": { "lat": 37.386, "lon": -122.0838 }
#     }
#   }
# }
# unifi.syslog.log.client_logs.unifisecuritygateway3p.messages.log
<filter unifi.syslog.**>
  @type    geoip
  # Specify optional geoip2 database
  geoip2_database   "/geoip/GeoLite2-City.mmdb"
  # Specify backend library (geoip2_c, geoip, geoip2_compat)
  backend_library geoip2_c
  geoip_lookup_keys  ["firewall.source.ip"]
  <record>
    geoip.latitude         ${location.latitude["firewall.source.ip"]}
    geoip.longitude        ${location.longitude["firewall.source.ip"]}
    # lat lon as properties
    # ex. {"lat" => 37.4192008972168, "lon" => -122.05740356445312 }
    geoip.location    '{ "lat" : ${location.latitude["firewall.source.ip"]}, "lon" : ${location.longitude["firewall.source.ip"]} }'
    # lat lon as string
    # ex. "37.4192008972168,-122.05740356445312"
    # geoip.location_string      ${location.latitude["firewall.source.ip"]},${location.longitude["firewall.source.ip"]}
    # GeoJSON (lat lon as array) is useful for Kibana's bettermap.
    # ex. [-122.05740356445312, 37.4192008972168]
    # geoip.location_array  '[${location.longitude["firewall.source.ip"]},${location.latitude["firewall.source.ip"]}]'
    # geoip.postal_code     ${postal.code["firewall.source.ip"]}
    geoip.region_name     ${subdivisions.0.names.en["firewall.source.ip"]}
    geoip.country_name    ${country.names.en["firewall.source.ip"]}
    geoip.country_iso_code ${country.iso_code["firewall.source.ip"]}
    geoip.city_name       ${city.names.en["firewall.source.ip"]}
  </record>
  # To avoid get stacktrace error with `[null, null]` array for elasticsearch.
  skip_adding_null_record  true
  # Set @log_level (default: warn)
  @log_level         debug
</filter>
<filter unifi.syslog.**>
  @type parser
  @id exclude_nil_firewall_source_ip
  key_name firewall_source_ip
  reserve_data true
  reserve_time true
  remove_key_name_field true
  emit_invalid_record_to_error false
  <parse>
    @type grok
    custom_pattern_path /grok.d
    <grok>
      pattern %{IP:remote_ip}
    </grok>
  </parse>
</filter>
# for debug
# <match kubernetes.var.log.containers.**fluentd**.log>
#   @type null
# </match>
# <match unifi.syslog.**>
#   @type stdout
# </match>
<filter access>
  @type record_transformer
  <record>
    hostname ${hostname}
  </record>
</filter>
# monitoring.conf: |-
# Prometheus Exporter Plugin
# input plugin that exports metrics
<source>
  @type prometheus
  bind 0.0.0.0
  port 24231
  metrics_path /metrics
</source>
<source>
  @type monitor_agent
</source>
# input plugin that collects metrics from MonitorAgent
<source>
  @type prometheus_monitor
  # update the metrics every 5 seconds
  interval 5
  <labels>
    host ${hostname}
  </labels>
</source>
# input plugin that collects metrics for output plugin
<source>
  @type prometheus_output_monitor
  # update the metrics every 5 seconds
  interval 5
  <labels>
    host ${hostname}
  </labels>
</source>
# input plugin that collects metrics for in_tail plugin
<source>
  @type prometheus_tail_monitor
  # update the metrics every 5 seconds
  interval 5
  <labels>
    host ${hostname}
  </labels>
</source>
# Concatenate multi-line logs
# <filter **>
#   @type concat
#   key message
#   multiline_end_regexp /\n$/
#   separator ""
# </filter>
<match unifi.syslog.**>
  @type copy
  <store>
    @id elasticsearch
    @type elasticsearch
    @log_level debug
    type_name fluentd
    include_tag_key true
    host elasticsearch
    port 9200
    logstash_format true
    logstash_prefix logstash-unifi
    index_prefix logstash-unifi
    reload_on_failure true
    reconnect_on_error true
    template_file /etc/fluent/conf.d/elasticsearch-template-es5x.json
    template_name elasticsearch-template-es5x.json
    <buffer>
      @type file
      flush_thread_count 8
      # WIP, need a place we can write to
      path /buffers/fluentd.centralized.system.buffer
      flush_mode interval
      retry_type exponential_backoff
      # flush_thread_count 2
      flush_interval 5s
      retry_forever
      retry_max_interval 30
      chunk_limit_size 16M
      queue_limit_length 256
      overflow_action block
    </buffer>
  </store>
  <store>
    @type stdout
    output_type json
  </store>
  <store>
    @type flowcounter
    @id applog_high_metrics
    tag fluentd.apploghigh.metrics
    aggregate all
    count_keys *
    unit minute
  </store>
</match>
